# ğŸ©º Modular RAG Medical AI Assistant

A production-ready **Retrieval-Augmented Generation (RAG) Medical Assistant** that enables users to upload medical documents (PDFs), retrieve context-specific information, and receive **factual, respectful, and safe answers**. The system follows a **modular architecture**, separating backend (FastAPI) and frontend (Streamlit).

âš ï¸ **Disclaimer:** This assistant is strictly informational and is explicitly instructed to **never provide medical advice**.

---

## âœ¨ Features

* **ğŸ“„ Document Upload & Embedding** â€“ Upload multiple PDF documents for analysis.
* **ğŸ” Contextual Retrieval (RAG)** â€“ Answers questions *only* from uploaded documents.
* **ğŸ§© Modular Architecture** â€“ Separate `server` (FastAPI) and `client` (Streamlit) for maintainability.
* **ğŸ¤– Precise LLM Guidance** â€“ Custom prompt ensures answers are factual, calm, and safe.
* **ğŸ’¬ Chat History** â€“ Chat history is stored and downloadable.
* **ğŸš€ Deployment Ready** â€“ Separate configurations for deploying backend and frontend.

---

## âš™ï¸ Technology Stack

| Component                 | Technology                   | Purpose                              |
| ------------------------- | ---------------------------- | ------------------------------------ |
| **Backend Framework**     | FastAPI                      | High-performance API server          |
| **Frontend Framework**    | Streamlit                    | Interactive web UI                   |
| **RAG / Orchestration**   | LangChain                    | Document processing & LLM chaining   |
| **LLM**                   | Groq Llama 3 (70B)           | Fast, large-scale text generation    |
| **Embedding Model**       | Google GenAI `embedding-001` | Vector embeddings (768-dim)          |
| **Vector Database**       | Pinecone                     | Cloud-native vector storage          |
| **Deployment (Backend)**  | Render                       | Hosting FastAPI backend              |
| **Deployment (Frontend)** | Streamlit Cloud              | Hosting Streamlit frontend           |
| **Dependencies**          | uv, pydantic, python-dotenv  | Package mgmt, validation, env config |

---

## ğŸš€ Getting Started

### 1. Prerequisites

Youâ€™ll need the following API keys:

* **Groq API Key** â€“ for Llama 3 model
* **Gemini API Key** â€“ for Google embeddings
* **Pinecone API Key & Environment** â€“ for vector DB

---

### 2. Clone & Setup

```bash
git clone https://github.com/AnjaliDhosariya/Medical-AI-Assistant
cd Medical-AI-Assistant

# Create and activate virtual environment
uv venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate      # On Windows
```

---

### 3. Configure Environment

Create a `.env` file in the project root and add your API keys:

```env
GROQ_API_KEY=your_groq_api_key
GEMINI_API_KEY=your_gemini_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_env
```

---

### 4. Install Dependencies

The project uses separate requirements for server and client.

```bash
# Install backend (server) dependencies
cd server
uv pip install -r requirements.txt
cd ..

# Install frontend (client) dependencies
cd client
uv pip install -r requirements.txt
cd ..
```

---

### 5. Run the Backend

```bash
cd server
uvicorn main:app --reload
```

Backend will run at: `http://127.0.0.1:8000`

---

### 6. Run the Frontend

```bash
cd client
streamlit run app.py
```

Frontend will open in your browser.

---

## ğŸ§‘â€ğŸ’» File Structure

```
Medical-AI-Assistant/
â”œâ”€â”€ client/                     # Frontend (Streamlit)
â”‚   â”œâ”€â”€ components/             # UI modules (Chat, Uploader, History)
â”‚   â”œâ”€â”€ utils/                  # API communication helpers
â”‚   â”œâ”€â”€ config.py               # API_URL configuration
â”‚   â”œâ”€â”€ app.py                  # Streamlit entry point
â”‚   â””â”€â”€ requirements.txt        # Dependencies for frontend
â”‚
â”œâ”€â”€ server/                     # Backend (FastAPI)
â”‚   â”œâ”€â”€ modules/                # Core RAG logic (LLM, Vector store, Handlers)
â”‚   â”œâ”€â”€ routes/                 # API endpoints (Upload, Ask)
â”‚   â”œâ”€â”€ middleware/             # Exception handlers
â”‚   â”œâ”€â”€ main.py                 # FastAPI entry point
â”‚   â””â”€â”€ requirements.txt        # Dependencies for backend
â”‚
â”œâ”€â”€ .env                        # API keys & config (ignored by Git)
â”œâ”€â”€ .gitignore                  # Git ignore file
â””â”€â”€ README.md                   # Project documentation
```

---

âœ… With this setup, you can upload medical PDFs, query them, and get **factual, context-specific responses** saf
